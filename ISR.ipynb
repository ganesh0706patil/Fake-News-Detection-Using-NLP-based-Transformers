{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNICpzrCq0BsLa6P0KBiEC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganesh0706patil/Fake-News-Detection-Using-NLP-based-Transformers/blob/main/ISR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8Zlyl0Wf_7A",
        "outputId": "199a5f99-eac1-4664-c098-51d51d24c48c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=101e861af762cee6761f6158c1b07e2de808d6bceac49b76bf74fee4b9179eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tax_N3cmgL7K",
        "outputId": "8b1b0637-3bcb-4f81-f344-df9acf9d3f9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2024.8.30)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=a3d890665ac31449df35c0704c2684c0149514430cd74e922d980b8078c7e12e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmUjyOdigFuV",
        "outputId": "76c90f20-ef3e-4071-d16c-2e26fbd11501"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import hashlib\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipedia\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the BERT model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")  # A lightweight BERT variant\n",
        "\n",
        "# Step 1: Setup Environment\n",
        "def create_topic_folder(topic):\n",
        "\n",
        "    main_folder = 'scraped_news'\n",
        "    topic_path = os.path.join(main_folder, topic)\n",
        "    if not os.path.exists(main_folder):\n",
        "        os.makedirs(main_folder)\n",
        "    if not os.path.exists(topic_path):\n",
        "        os.makedirs(topic_path)\n",
        "\n",
        "# Step 2: Fetch News Articles Using News API\n",
        "def fetch_news_articles_newsapi(topic, search_query, api_key, page_size=15):\n",
        "\n",
        "    base_url = 'https://newsapi.org/v2/everything'\n",
        "    params = {\n",
        "        'q': search_query,\n",
        "        'apiKey': api_key,\n",
        "        'pageSize': page_size,\n",
        "        'language': 'en',\n",
        "        'sortBy': 'relevancy'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        articles = data.get(\"articles\", [])\n",
        "\n",
        "        if not articles:\n",
        "            print(\"No articles found for the given query.\")\n",
        "            return\n",
        "\n",
        "        for article in articles:\n",
        "            title = article.get('title', 'No Title')\n",
        "            description = article.get('description', 'No Description')\n",
        "            content = article.get('content', 'No Content')\n",
        "            url = article.get('url', 'No URL')\n",
        "\n",
        "            # Fetch and use full content from the article URL\n",
        "            full_content = fetch_full_content(url)\n",
        "\n",
        "            if full_content == \"Content not available\":\n",
        "                print(f\"Skipping article due to content retrieval failure: {url}\")\n",
        "                continue\n",
        "\n",
        "            article_text = f\"Title: {title}\\nDescription: {description}\\nContent: {full_content}\\nURL: {url}\\n\"\n",
        "\n",
        "            save_article(topic, article_text, url)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching articles from News API: {e}\")\n",
        "\n",
        "def fetch_news_articles_newsdata(topic, search_query, api_key, page_size=15):\n",
        "\n",
        "    base_url = 'https://newsdata.io/api/1/news'\n",
        "    params = {\n",
        "        'q': search_query,\n",
        "        'apikey': api_key,\n",
        "        'language': 'en',\n",
        "        'page': 1,\n",
        "        'pageSize': page_size,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        articles = data.get(\"results\", [])\n",
        "\n",
        "        if not articles:\n",
        "            print(\"No articles found for the given query.\")\n",
        "            return\n",
        "\n",
        "        for article in articles:\n",
        "            title = article.get('title', 'No Title')\n",
        "            description = article.get('description', 'No Description')\n",
        "            content = article.get('content', 'No Content')\n",
        "            url = article.get('link', 'No URL')\n",
        "\n",
        "            # Fetch and use full content from the article URL\n",
        "            full_content = fetch_full_content(url)\n",
        "\n",
        "            if full_content == \"Content not available\":\n",
        "                print(f\"Skipping article due to content retrieval failure: {url}\")\n",
        "                continue\n",
        "\n",
        "            article_text = f\"Title: {title}\\nDescription: {description}\\nContent: {full_content}\\nURL: {url}\\n\"\n",
        "\n",
        "            save_article(topic, article_text, url)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching articles from NewsData.io API: {e}\")\n",
        "\n",
        "def fetch_full_content(url):\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 403:\n",
        "            print(f\"Access forbidden (403): {url}\")\n",
        "            return \"Content not available\"\n",
        "        elif response.status_code != 200:\n",
        "            print(f\"Failed to retrieve article content: {response.status_code}\")\n",
        "            return \"Content not available\"\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "        return full_text if full_text else \"Content not available\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching article content: {e}\")\n",
        "        return \"Content not available\"\n",
        "\n",
        "def save_article(topic, content, url):\n",
        "\n",
        "    file_name = hashlib.md5(url.encode()).hexdigest() + '.txt'\n",
        "    path = os.path.join('scraped_news', topic)\n",
        "    with open(os.path.join(path, file_name), 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "\n",
        "# Step 3: Retrieve Full Wikipedia Page Content\n",
        "def fetch_wikipedia_page(query):\n",
        "\n",
        "    try:\n",
        "        page = wikipedia.page(query)\n",
        "        return page.content\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        return f\"Disambiguation Error: {e.options}\"\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        return \"No Wikipedia page available for the given query.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error retrieving Wikipedia page: {e}\"\n",
        "\n",
        "def save_wikipedia_page(content, topic):\n",
        "\n",
        "    file_name = 'wiki.txt'\n",
        "    path = os.path.join('scraped_news', topic)\n",
        "    with open(os.path.join(path, file_name), 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "\n",
        "# Step 4: Preprocess Text\n",
        "def preprocess_text(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Step 5: Calculate Similarity\n",
        "def calculate_similarity(document, query):\n",
        "    # Encode the document and query\n",
        "    document_embedding = model.encode(document, convert_to_tensor=True)\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = util.pytorch_cos_sim(document_embedding, query_embedding)\n",
        "    return similarity.item()\n",
        "\n",
        "# Step 6: Main Function to Coordinate Fetching, Saving, Ranking, and Retrieving\n",
        "def main():\n",
        "    search_query = \"October 2024 Iranian strikes against Israel\"\n",
        "    # \"Modi appeals for safety of Hindus in crisis-hit Bangladesh\"\n",
        "    # \"October 2024 Iranian strikes against Israel\"\n",
        "    # \"Maharashtra Assembly Elections on 20 November\"\n",
        "    # \"Chandrayaan-3 Landed on the Moon\"\n",
        "    # \"By next year we will able live on sun’\"\n",
        "    topic = \"space\"\n",
        "    create_topic_folder(topic)\n",
        "\n",
        "    # Replace with your News API keys\n",
        "    news_api_key = 'News API Key'\n",
        "    newsdata_io_api_key = 'NewsDataio API Key'\n",
        "\n",
        "    # Fetch news using News API\n",
        "    fetch_news_articles_newsapi(topic, search_query, news_api_key)\n",
        "\n",
        "    # Fetch news using NewsData.io API\n",
        "    fetch_news_articles_newsdata(topic, search_query, newsdata_io_api_key)\n",
        "\n",
        "    # Retrieve Wikipedia page content and save it\n",
        "    wiki_content = fetch_wikipedia_page(search_query)\n",
        "    save_wikipedia_page(wiki_content, topic)\n",
        "    print(f\"Wikipedia page content for '{search_query}' has been saved to wiki.txt\")\n",
        "\n",
        "    # Calculate similarity for each article in the folder\n",
        "    articles_path = os.path.join('scraped_news', topic)\n",
        "    similarity_scores = []\n",
        "\n",
        "    for file_name in os.listdir(articles_path):\n",
        "        if file_name.endswith('.txt') and file_name != 'wiki.txt':\n",
        "            file_path = os.path.join(articles_path, file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                article_content = f.read()\n",
        "\n",
        "            # Preprocess text\n",
        "            preprocessed_article = preprocess_text(article_content)\n",
        "            preprocessed_wiki = preprocess_text(wiki_content)\n",
        "\n",
        "            # Calculate similarity\n",
        "            similarity_score = calculate_similarity(preprocessed_article, preprocessed_wiki)\n",
        "            similarity_scores.append((file_name, similarity_score))\n",
        "\n",
        "    # Rank articles by similarity score (highest to lowest)\n",
        "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    threshold = 0.4  # Define a similarity threshold for deciding truthfulness\n",
        "    a = 0\n",
        "    b = 0\n",
        "\n",
        "    print(\"\\nRanked Articles by Similarity Scores:\")\n",
        "    for file_name, score in similarity_scores:\n",
        "        if score > threshold:\n",
        "            print(f\"Article {file_name} is likely true (Similarity Score: {score:.6f}).\")\n",
        "            a += 1\n",
        "        else:\n",
        "            print(f\"Article {file_name} is likely fake (Similarity Score: {score:.6f}).\")\n",
        "            b += 1\n",
        "    if a > b:\n",
        "      print(\"The news is true\")\n",
        "    elif a < b:\n",
        "      print(\"The news is fake\")\n",
        "    else:\n",
        "      print(\"Cannot be determined\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O2pDb02gFxX",
        "outputId": "875b06cf-9719-459f-be1d-e1dbb8b829c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching articles from NewsData.io API: 422 Client Error: UNPROCESSABLE ENTITY for url: https://newsdata.io/api/1/news?q=October+2024+Iranian+strikes+against+Israel&apikey=pub_5234266b89bfb262b75fa558ee28b80433d18&language=en&page=1&pageSize=15\n",
            "Wikipedia page content for 'October 2024 Iranian strikes against Israel' has been saved to wiki.txt\n",
            "\n",
            "Ranked Articles by Similarity Scores:\n",
            "Article df1f6fd2eb25fac0e8618d969c006b31.txt is likely true (Similarity Score: 0.773613).\n",
            "Article 5ec34d409120e3b8437baedf3ae37d5e.txt is likely true (Similarity Score: 0.763380).\n",
            "Article 2fdeb6d252a6120df97c5e669f6cb147.txt is likely true (Similarity Score: 0.761404).\n",
            "Article 747a6232e30087c6a14c1a571442f1c7.txt is likely true (Similarity Score: 0.723257).\n",
            "Article ea5d31c62467d3ba6a1cdbc5dec88f9c.txt is likely true (Similarity Score: 0.701456).\n",
            "Article 63a3c4988db34a23e2776db23fed5a6d.txt is likely true (Similarity Score: 0.693264).\n",
            "Article 582cbdfbce7616f031239bf99121d399.txt is likely true (Similarity Score: 0.671634).\n",
            "Article 37d4cc2fa6c86407d04476c2e1050e4f.txt is likely true (Similarity Score: 0.660672).\n",
            "Article 42cc60953924d56e503148f3ca5fd3e2.txt is likely true (Similarity Score: 0.616063).\n",
            "Article 28672b045b04d2f95a09b3887f3e4c41.txt is likely true (Similarity Score: 0.563412).\n",
            "Article e9de337aba7e6e94df914ef911f57120.txt is likely true (Similarity Score: 0.541113).\n",
            "Article c241e8a9a3e6c4012eb9f003b8d1c6f5.txt is likely true (Similarity Score: 0.540222).\n",
            "Article 5d52153e0dd2d873fc210af8856a02ad.txt is likely true (Similarity Score: 0.531037).\n",
            "Article ef3049ef0445328b6705c98bbbb885ad.txt is likely true (Similarity Score: 0.448473).\n",
            "Article b81c4af459991d9dcba0cd5b8734531a.txt is likely fake (Similarity Score: 0.253246).\n",
            "The news is true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1-wwbivlgFy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rcVvRVXPgF1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rrHSZoTdgF3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}